---
output:
  html_document:
          number_sections: true
          theme: cosmo
---


# Executive Summary

# Data Processing

## Acquire Data



```{r message=FALSE}
#Load Libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(caret)
library(tibble)
library(plotly)
library(reshape2)
library(corrplot)
library(parallel)
library(doParallel)
```
```{r}
#Aquire Data
setwd("E:/Google Drive/Data Science/Coursera Data Science/Course 8")
mainData <- read.csv("mainData.csv") %>% as_tibble()
predData <- read.csv("predData.csv") %>% as_tibble()
```

Data acquired from : http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz5VsmSupeQ

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

The data consists of 2 .csv files: mainData.csv which holds the raw experimental data and predData.csv which holds the prediction questions for the project.

The intention is to use mainData.csv to form the training and testing sets on which the modelling will take place and to use the model to predict the outcomes for the data in predData.csv.

## File Structure and Data Types


```{r}
#review file structures
namesComp <- names(mainData) == names(predData) 
summary(namesComp)
which(namesComp == FALSE)

nameDiff <- c(head(mainData, 5)[,160],head(predData, 5)[,160])
nameDiff %>% as.data.frame()
```

Comparing the 2 .csv files we see that the column names (features) are the same apart from one - feature 160, "classe" of the exercise on the mainData.csv and "problem_id" on the predData.csv. As these represent the dependent variable and prediction question for the dependent variable I am satisfied the data is structured alike in both files. 

This does mean, however, that any data transformations must be carried out on both files to allow for accurate prediction following modelling.

```{r results='hide', message=FALSE, warning=FALSE}
##review data types
str(mainData)
dataType.main <- sapply(mainData, class) %>% 
        as.data.frame() %>% 
        rownames_to_column()

str(predData)
dataType.pred <- sapply(predData, class) %>% 
        as.data.frame() %>% 
        rownames_to_column()


#amend data types
mainData.num <- mainData
mainData.num[, 8:159] <- sapply(mainData.num, function(x) as.numeric(as.character(x)))

predData.num <- predData
predData.num[, 8:159] <- sapply(predData.num, function(x) as.numeric(as.character(x)))
```

Reviewing the structure of the data it is apparent that the vast majority refers to the sensor data taken during the lifting exercises and, as such, should be treated as a numeric variable. Columns 8:159 changed to numerical data on both files in relation to this.

## Missing Values

```{r}
#removing NAs from training set
mainDataClean <- mainData.num
mainDataClean <- mainDataClean %>% select_if(~ !any(is.na(.)))

predDataClean <- predData.num
predDataClean <- predDataClean %>% select_if(~ !any(is.na(.)))

#collate remaining shared features
mainCols <- names(mainDataClean)
predCols <- names(predDataClean)

sharedCols <- intersect(mainCols, predCols)
mainDataShared <- mainDataClean[,sharedCols]
mainDataShared <- cbind(mainDataShared, mainDataClean[,57])
```

The missing data forms the vast majority of the content of the columns in which it is found. There is insufficient data within the columns to attempt imputation. As such, column removal chosen to resolve missing data issues. 

In order to allow prediction to take place, the features of the training/testing data must be a subset of the features of the prediction data. To allow this I have subset the data of the features shared across the training/testing and prediction dataframes, whilst leaving the dependent variable (classe) attached to allow modelling. 

This means that, in essence, 103 of 160 features have been removed from the training/testing data at this stage due to missing data between the datasets.

## Feature Selection

```{r cache=TRUE}
#check for outliers
numericVars <- Filter(is.double, mainDataShared) 
outliers <- numericVars %>% sapply(function(x) boxplot(x, plot=FALSE)$out)
outliersAmount <- summary(outliers) %>% as.data.frame()
outliersAmount <- outliersAmount %>% filter(Var2 == "Length") 
outliersAmount$n <- as.numeric(as.character(outliersAmount$Freq))
outliersAmount <- outliersAmount%>% mutate(percent = n/nrow(numericVars)*100)

#near zero variance
nzv <- mainDataShared %>% nearZeroVar(saveMetrics = TRUE) %>% rownames_to_column()
nzvRemove <- nzv %>% filter(nzv == TRUE)
nzvRemove <- nzvRemove[1,1] 
mainDataShared <- mainDataShared %>% select(-nzvRemove)

#correlation & colinearity
corrMatrix <- numericVars %>% cor()
highCorr <- findCorrelation(corrMatrix, cutoff = 0.85)
exploreData <- mainDataShared[,-highCorr]

#remove timestamp/ID features
finalData <- exploreData %>% select(-X, -user_name, -raw_timestamp_part_1, -raw_timestamp_part_2, -roll_belt)
```

There are significant numbers of outliers (defined using Tukeys method). As these outliers form significant percentages of the data I have chosen to leave them in. I theorise that these could represent the measurements of the active parts of the exercise.

The features with near zero variance have been removed and those features with a greater correlation than 0.85 have been removed to reduce any potential issues with colinearity.

Finally, the timestamp and ID features have been removed. Roll_belt removed as was same as row number.

# Modelling

## Preparatory Steps

```{r}
#create training/testing sets
set.seed(111)
trainIndex <- createDataPartition(finalData$classe, p = 0.75, list = FALSE)
training <- finalData[ trainIndex,]
testing  <- finalData[-trainIndex,]

#Set up parallel processing
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

#Cross Validation 5 fold
fitControl<- trainControl(method = "cv", number = 5, savePredictions = TRUE, allowParallel = TRUE)
```

Split data 75% training, 25% testing and apply 10 fold cross validation.

## Models

### Support Vector Machine
```{r cache=FALSE}
#modelling SVM
system.time(model.SVM.classe <- train(classe ~ ., data = training, method = "svmLinear", trControl = fitControl))
model.SVM.classe

#Predictions on the test set (model = model name, testing = test set)
predictions.SVM.classe <- predict(model.SVM.classe, testing)
testing$predictions.SVM.classe <- predictions.SVM.classe 

#Confusion matrix
confMatrix.SVM <- confusionMatrix(testing$classe, testing$predictions.SVM.classe)
confMatrix.SVM$table
postResample(testing$predictions.SVM.classe, testing$classe)
```

The Accuracy of the model on the test set was 0.7245106, this means that the out of bag error is 0.275.

### Random Forest

```{r cache=FALSE}
#modelling RF
set.seed(111)
system.time(model.RF.classe <- train(classe ~ ., data = training, method = "rf", trControl = fitControl))
model.RF.classe

#Predictions on the test set (model = model name, testing = test set)
predictions.RF.classe <- predict(model.RF.classe, testing)
testing$predictions.RF.classe <- predictions.RF.classe 

#Confusion matrix
confMatrix.RF <- confusionMatrix(testing$classe, testing$predictions.RF.classe)
confMatrix.RF$table
postResample(testing$predictions.RF.classe, testing$classe)
```

The Accuracy of the model on the test set was 0.9995922, this means that the out of bag error is 0.0004.

### Decision Tree

```{r cache=FALSE}
#modelling DT
system.time(model.DT.classe <- train(classe ~ ., data = training, method = "rpart", trControl = fitControl))
model.DT.classe

#Predictions on the test set (model = model name, testing = test set)
predictions.DT.classe <- predict(model.DT.classe, testing)
testing$predictions.DT.classe <- predictions.DT.classe 

#Confusion matrix
confMatrix.DT <- confusionMatrix(testing$classe, testing$predictions.DT.classe)
confMatrix.DT$table
postResample(testing$predictions.DT.classe, testing$classe)
```

The Accuracy of the model on the test set was 0.5193719, this means that the out of bag error is 0.481.

### Gradient Boosting Machine

```{r cache=FALSE}
#modelling GBM
set.seed(111)
system.time(model.GBM.classe <- train(classe ~ ., data = training, method = "gbm", trControl = fitControl, verbose = FALSE))
model.GBM.classe

#Predictions on the test set (model = model name, testing = test set)
predictions.GBM.classe <- predict(model.GBM.classe, testing)
testing$predictions.GBM.classe <- predictions.GBM.classe 

#Confusion matrix
confMatrix.GBM <- confusionMatrix(testing$classe, testing$predictions.GBM.classe)
confMatrix.GBM$table
postResample(testing$predictions.GBM.classe, testing$classe)
```

The Accuracy of the model on the test set was 0.9967374, this means that the out of bag error is 0.003.

## Modelling conclusions

```{r}
##Turn off parallel processing
stopCluster(cluster)
registerDoSEQ()
```

The Random Forest had the best metrics with the lowest out of bag error of 0.0004.

## Prediction Exercise



# Conclusions